# search tweet_num/2 tweets by hashtag & tweet_num/2 by word (25 if tweet_num = 50)
def get_tweets(keyword=None, hashtag=None, lang=None, tweet_num=None):
    # query term
    if keyword is not None:
        query_term = keyword
    elif hashtag is not None:
        query_term = "#" + hashtag
    else:   # keyword is None & hashtag is None:
        query_term = "#VaccinationDrive"
    # print(f"query_term = {query_term}")
    # print(f"lang = {lang}")

    # get word without hashtag
    if query_term[0] != "#":
        word = query_term
    else:
        word = query_term[1:]   # remove hastag
    # print(f"word = {word}")   # D

    # lib = ""

    # set default value for tweet_num
    # tweet_num = 3   # OPT: 50
    if (tweet_num == None):
        tweet_num = 3

    try:   # default is tweepy lib
        lib = "tweepy"
        from packages.scrape_tweet_by_tweepy import main
        out = main(word, tweet_num, lang)
    except:  # switch to snscrape lib (if tweepy is down)
        lib = "snscrape"
        from packages.scrape_tweet_by_snscrape import main
        out = main(word, tweet_num)
    # print(f"=== lib used: {lib} ===")   # D

    """# TEST CASE: add stopword into text
    for output in out:
        print(f'BFR: output["content"] = {output["content"]}')
        output["content"] += "å¤©åŸç‡éŸ³ã§ã‚ã‚Œã°ã€ã€Œå¤©åŸç‡éŸ³ã€ã€Œå¤©åŸã€ã€Œç‡éŸ³ã€ãªã©è‡ªèº«ã®åå‰ã¯çµ¶å¯¾1ç•ªãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ä¸Šã§å¤§ããè¡¨ç¤ºã•ã‚Œã¦ã—ã¾ã†ã¯ãšãªã®ã§ã€å„ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è‡ªèº«ã®åå‰ã¯è¡¨ç¤ºã—ãªã„ã‚ˆã†ã«ã—ãŸã„ã§ã™"
        print(f'AFR: output["content"] = {output["content"]}')"""

    # TODO: create a new folder, namely tweets_backup (if it is no exist yet)
    output_filename = "tweets_backup/" + \
        word.replace(" ", "_") + "_" + lib + ".txt"   # OPT: .json
    # print(f"=== create file: {output_filename} ===")   # D
    with open(output_filename, 'w') as f:
        # print(f"out = {out}")   # D
        # f.write(f"=== lib used: {lib} ===\n\nout = ")   # OPT: tweet.json()
        f.write(str(out))   # OPT: tweet.json()
        f.write('\n')
        f.flush()

    # print(f"OUT: out = {out}")   # D
    return out


def get_text(keyword=None, hashtag=None, lang=None):
    tweets = get_tweets(keyword, hashtag, lang)

    text = ""
    for tweet in tweets:
        # print(f"IMPORTANT: tweet = {tweet}")
        text += tweet._json["full_text"]
        # print("text:", text)

    return text


# TODO: rename "title" to "content"
# def get_data(keyword=None, hashtag=None, lang=None, prev_data=None):
def get_data(tweets, lang):
    import fugashi
    from wordcloud import STOPWORDS

    # create stopword list:
    stopwordSet = set(STOPWORDS)   # maybe can set at outer level
    if lang == "en":
        stopwordSet.update(["drink", "now", "wine", "flavor", "flavors"])
    elif lang == "ja":
        # "ã†", "ã‡", "ãŠ", "ã", "ã”", "ã£", "ã²", "ã¾", "ã‚‡", "ã‚‰", "å‡º", "å¼¦", "æ–‘", "æ–¹", "ç¨‹", "è¦‹", "é¡˜",
        # "ã–", "ã ", "ã¡", "ã¬", "ã‚Š", "ä¼", "å®œ", "æƒ", "æœ", "æ¬²", "ç€",
        #  "ã", "ã’", "ã ", "ã»", "ã‚",
        # "äºº", "ä»•", "ä½¿", "å¥ˆ", "å³¶", "æ‚Ÿ", "æ„›", "æ™’", "æ¯›", "ç‹™", "ç‹¼", "ç¸", "è²·", "è¸Š", "é¦¬",
        # "ã†", "ãŠ", "ã", "ã£", "ã¾", "ã¿", "ã‚„", "ã‚ˆ", "ã‚­", "ãƒ‹", "ä¸‰", "å³", "å›", "å¤¢", "å¬‰", "æ—§", "æ¨¹", "æµ", "æ¸‰", "ç–²", "çš„", "ç§", "ç¾", "è‰²", "è¦‹",
        stopwordSet.update([
            # 2) from ng col (https://docs.google.com/spreadsheets/d/1RF3nzLGfaQeENwMXYQOR3Dw6bH7l7dJ9a6IPlbYW6l8/edit?fbclid=IwAR3W1apupfK4UJO-PtmszOF3Cuf9PCI5FNQIluY_JFWgWQ_lRkGLy1e1HMU#gid=235398883)
            "æ—¥æœ¬é¬¼å­", "ã‚ã‚“ã‚¹ã‚¿", "ã‚ã‚“ã•ã‚“ã¶ã‚‹ã‚¹ã‚¿ãƒ¼ã‚º", "ã‚ãƒ¼ã‚€ã‹ã£ã¨", "ã‚ãˆã„ã§", "ã‚ã—ãã‚Š", "ã‚ã™ãº", "-ã‚ã™ãºãã¨", "ã‚ã©ã‚Œã™", "ã‚ã¬ã‚‰ãƒ¼", "ã‚ã‚å…¬", "ã‚ã‚‹ã¡ã‚…ã†", "ã‚ã‚‹ä¸­", "ã„ãŸã“ã†", "ã„ãŸå…¬", "ã„ã‚“ã°ã„", "ãƒ´ãããª", "ã†ã‡ã®ã‚€", "ã†ã–ã„", "ã†ã˜ã‚ˆã‚Šãã ã¡", "ãˆã¡ã”ã®ã“ã‚ã¤ã", "ãˆã£ã¡", "ãˆã‚“ã˜ã‚‡ã“ã†ã•ã„", "ãŠãƒ¼ãŒãšã‚€", "ãŠãƒ¼ã‚‹ã©ã¿ã™", "ãŠãªã«", "ãŠãªã¬", "ãŠãªã»", "ãŠã«ã—ã¾ã¥", "ãŠã¼ã‚Œã˜ã¬", "ãŠã¾ã‚“ãŸã‚“", "ãŠã‚ã“", "-ã‚ã‘ãŠã‚ã“ã¨ã‚ˆã‚", "ãŠã‚ã—ã‚ƒã‚“", "ãŠã‚ã‚“ã¡ãƒ", "ãŠã‚‚ã¦ã«ã»ã‚“", "ãŠã‚ã„å±‹", "ãŠã‚“ãªã“ã©ã‚‚", "ã‹ã„ã—ã‚…ã‚“", "ãŒã„ã˜", "-ãŒã„ã˜ã‚“", "ã‹ã†ã±ãƒ¼", "ã‹ãã—ã©ã‚Š", "ã‹ãã¡ã‚‡ã†ã„ã‚“", "ã‹ãã¡ã‚‡ã†ã ã‚“", "ã‹ã“ã¡ã‚‡ã†", "ãŒã¡ã‚ƒç›®", "ã‹ã¤ãå±‹", "ã‹ã¶ã‚„", "ã‹ã‚ã‚‰ã“ã˜ã", "ã‹ã‚“ãã‚“", "ãŒã‚“ã˜ã‚ƒ", "ã‹ã‚“ãŸã‚Šã˜ã‚“", "ãã‹ã‚Œã‚‚ã®", "ãã£ã¡ã‚‡", "ãã‚ãˆ", "ãã‚ƒãã•ã¤", "ãã‚ƒããŸã„", "ãã‚ƒãã¯ã‚“", "ãã‚‡ã†ã›ã„ã‚ã„ã›ã¤", "ãã‚‡ã†ã¯ã", "ãã‚“ã—ã‚“ãã†ã‹ã‚“", "ãå°", "ãã„ã¡ã‚“æ£’", "ãã©ã‚“", "ãã±ã", "ãã³ãã‚Š", "ãã‚“ã«ã‚Šã‚“ãã™", "ãã‚“ã‚‚ã†", "ã“ãƒ¼ã‚‹ãŒãƒ¼ã‚‹", "ã“ã‹ã„ã‚“", "ã“ã£ã±ã‚„ãã«ã‚“", "ã”ã¿è™«", "ã“ã‚ã™", "ã“ã‚ã›", "ã”ç„¡", "ã”æœ‰", "ã–ãƒ¼ã‚ã‚“", "ã•ã¤ã˜ã‚“", "ã•ã‚“ã‹ã‚“ã›ã„ã°ã¤", "ã•ã‚“ã”ãã˜ã‚“", "ã—ã“ã£ã¦", "ã—ã£ãã™ãªã„ã‚“", "ã—ã®ã†ã“ã†ã—ã‚‡ã†", "ã˜ã¸ã„ã—ã‚‡ã†ã˜", "ã˜ã‚…ã†ã‹ã‚“", "ã—ã‚…ã†ã’ã", "ã—ã‚…ã†ã›ã‚“ã‚„", "ã—ã‚‡ã†ã„ã‚“ã—ã‚“", "ã—ã‚‡ãã¶ã¤ã«ã‚“ã’ã‚“", "ã—ã‚Šã¬ãã„", "ã—ã‚“ãã‚“ãã†ã‹ã‚“", "ã—ã‚“ã—ã‚“ã—ã‚‡ã†ãŒã„ã—ã‚ƒ", "ã˜ã‚“ã—ã‚“ã°ã„ã°ã„", "ã—ã‚“ã¸ã„ã¿ã‚“", "ãšãƒ¼ã—ã‚ƒãƒ¼", "ãšãƒ¼ã˜ã‚ƒãƒ¼", "ã™ããƒ¼ã‚‹æ°´ç€", "ã™ã‘ã¾ã‚“", "ã™ã¨ãƒ¼ã‹ãƒ¼", "ã™ã¨ãƒ¼ãã‚“ã", "ã™ãºã—ã‚ƒã‚‹k", "ã™ãºã‚‹ã¾", "ã™ã‚ã£ã´ã‚“ã", "ã›ã„ã—ã‚“ã„ã˜ã‚‡ã†", "ã›ã„ã—ã‚“ã¯ãã˜ã‚ƒãã˜", "ã›ã„ã¯ãã—ã‚ƒ", "ã›ãã—ã‚ƒã‚‹ã¯ã‚‰ã™ã‚ã‚“ã¨", "ããƒ¼ã·", "ãã ã¡ã‚ˆã‚Šã†ã˜", "ãã¡ã‚“", "ãã‚Šã¾ã‚“", "ã ã„ã„ã‚“ã—ã‚“", "ã ã„ã•ã‚“ã”ã", "ã ã„ã‚ã‚“ã¯ã’", "ãŸã‘ã®ã“ã„ã—ã‚ƒ", "ãŸã‘ã®ã“åŒ»è€…", "ãŸã“éƒ¨å±‹", "ãŸã ã¾ã‚“", "ãŸã¡ã‚ãŒã‚Œæ—¥æœ¬", "ãŸã¡ã‚“ã¼", "ãŸã²ã­", "ãŸã‚Œæµã—", "ã¡â—‹ã“", "ã¡ã‹ã‚“", "ã¡ãã³", "ã¡ãã“", "ã¡ãã¡ã", "ã¡ãã½", "ã¡ã‚ƒã‚“ã“ã‚", "ã¡ã‚…ã†ãã‚‡ã†", "ã¡ã‚‡ã†ã›ã‚“ã˜ã‚“", "ã¡ã‚‡ã†ã›ã‚“ã›ã„ã°ã¤", "ã¡ã‚‡ã£ã±ã‚Š", "ã¡ã‚“ã‹ã™", "ã¡ã‚“ã“", "ã¡ã‚“ã—ã“", "ã¡ã‚“ã¡ã‚‡ã†", "ã¡ã‚“ã¼ã†", "ã¡ã‚“é•·", "ã¤ã‚“ã¼", "ã¦ã„ã‹ã„ã¯ã¤ã“ã", "ã§ã‹ã¡ã‚“", "ã§ã‚Šã¸ã‚‹", "ã¦ã‚Œãã‚‰", "ã¦ã‚“ã•ã„ã¨ãã‚‡ã†ã˜ã‚“ã¯ã‹ã¿ã²ã¨ãˆ", "ã©ãƒ¼ã¦ãƒ¼", "ã¨ã†ã‚ã³ã‚‡ã†ãµ", "ã¨ã†ã•ã¤", "ã¨ã†ã¡ã‚‡ã†", "ã©ã†ã¦ãƒ¼", "ã©ã†ã¦ã„", "ã¨ãã—ã‚…ãŒã£ãã‚…ã†", "ã¨ãã—ã‚…ãŒã£ã“ã†", "ã¨ãã—ã‚…ã¶ãŸã„", "ã¨ãã—ã‚…ã¶ã‚‰ã", "ã©ã–ãˆã‚‚ã‚“", "ã¨ã•ã¤ã˜ã‚‡ã†", "ã¨ã•ã¤ã«ã‚“", "ã©ã•ã¾ã‚ã‚Š", "ã©ã•å›ã‚Š", "ã©ã´ã‚…", "ã©ã‚‚ã‚Š", "ã©ã‚„ãŒã„", "ã¨ã‚„ã¾ã®ã•ã‚“ã™ã‘", "ã©ã‚„è¡—", "ã¨ã‚‹ã“ã˜ã‚‡ã†", "ã¨ã‚‹ã“ã¶ã‚", "ã¨ã‚‹ã“å¬¢", "ã¨ã‚‹ã“é¢¨å‘‚", "ã©ã‚“ç™¾å§“", "ãªã¾ã»ã‚“ã°ã‚“", "ãªã¾æœ¬ç•ª", "ã«ãã‚", "ã«ã“ã‚ˆã‚“", "ã«ã‚“ã´ã«ã‚“", "ã¬ã‚Œã¾ã‚“", "ã®ã†ãªã—", "ã®ã†ã¾ããˆã‚“", "ã°ãƒ¼ã¦ã„", "ã¯ã„ã˜ã‚ƒã£ã", "ã°ã„ã—ã‚…ã‚“", "ã±ã„ãšã‚Š", "ã±ã„ã±ã‚“", "ã°ã„ã¶", "-ã‚Šã°ã„ã¶", "ã°ã‹ã¡ã‚‡ã‚“", "ã±ã“ã±ã“", "ã°ã™ã˜ã‚ƒã£ã", "ã°ãŸã‚„", "ã°ãŸå±‹", "ã¯ã£ãã‚‡ã†", "ã°ã£ãŸå±‹", "ã¯ã¿ã¡ã‚“", "ã¯ã‚æ’®ã‚Š", "ã±ã‚“ã¤", "-ã±ã‚“ã¤ããƒ¼", "ã±ã‚“ã¦ãƒ", "ã¯ã‚“ã¨ã†ã˜ã‚“", "ã´ãƒ¼ã´ã‚“ã", "ã´ã£ãã‚“ã", "ã³ã£ã¡", "ã²ã®ã‚‚ã¨ãŠã«ã“", "ã²ã‚“ã«ã‚…ãƒ¼", "ã²ã‚“ã«ã‚…ã†", "ã²ã‚“ã®ã†", "ãµãƒ¼ãã", "ãµãã£ã", "ãµã†ãã", "ãµã‡ã‚‰", "-ã™ãµã‡ã‚‰", "-ãµã‡ã‚‰ãƒ¼ã‚Š", "-ã‹ãµã‡ã‚‰ã¦", "-ã‚ã£ããµã‡ã‚‰ãƒ¼", "-ãµã‡ã‚‰ãŒã‚‚", "ã¶ã•ã„ã", "ãµã˜ã®ã‚„ã¾ã„", "ã·ã£ã—ãƒ¼", "ã¶ã£ã—ã‚…ã¾ã‚“", "ãµã¿ãã‚Šã°ã‚“", "ã¶ã‚‰ãã¿ã‚“", "ã¶ã‚‹ã›ã‚‰", "ãµã‚ã†ã˜", "ãµã‚ã†ã—ã‚ƒ", "ã¶ã‚“ã‚‚ã†", "ãºãƒ¼ã«ã™", "ãºã„æ‚£", "ãºã«ãƒ¼ã™", "ãºã«ã™", "ã¸ã‚ã„ã‚“", "ã»ã†ã‘ã„", "ã¼ã†ã“ã†", "ã½ã“ã¡ã‚“", "ã½ã“ãºã‚“", "ã¼ã£ã", "ã»ã¦ã¨ã‚‹", "ã½ã‚Šå…¬", "ã½ã‚“ã“ã¤å±‹", "ã½ã‚“ã³ã", "ã½ã‚“å¼•ã", "ã»è¾¼", "ã»åˆ¥", "ã¾â—‹ã“", "ã¾ã˜ãã¡", "ã¾ã™ãŸãƒ¼ã¹ãƒ¼ã—ã‚‡ã‚“", "ã¾ãã“", "ã¾ã‚Šãµããª", "ã¾ã‚ã™", "ã¾ã‚“â—‹", "ã¾ã‚“ã‹ã™", "ã¾ã‚“ãã‚“", "ã¾ã‚“ãã‚Š", "ã¾ã‚“ã“", "ã¾ã‚“ã”ã£ã“", "ã¾ã‚“ãšã‚Š", "ã¾ã‚“ãŸã", "ã¾ã‚“ã¡ã¤", "ã¾ã‚“ã¡ã‚“", "ã¾ã‚“ã´ãƒ¼", "ã¾ã‚“ã³ã", "ã¾ã‚“ã³ã‚‰", "ã¾ã‚“é–‹", "-ã‚ã‚“ã¾ã‚“é–‹å‚¬", "ã¾ã‚“é‡‘", "ã¾ã‚“æ±", "ã¾ã‚“æ‹“", "ã¾ã‚“æ¯›", "ã¿ã‹ã„ã¯ã¤ã“ã", "ã¿ã¤ãã¡", "ã¿ã¤ã°ã„", "ã¿ã‚“ãªã®å…š", "ã‚€ã“ã‚’ã¨ã‚‹", "ã‚€ã™ã‚ã‚’ã‹ãŸã¥ã‘ã‚‹", "ã‚ã‹ã‘", "ã‚ãã‚‰ã°ã‚“", "ã‚ãã‚‰ç¸", "ã‚ãã‚‰è›‡ã«ãŠã˜ãš", "ã‚ãã‚‰åˆ¤", "ã‚ãã‚‰æ»…æ³•", "ã‚ã‚‹ã‚ã©", "ã‚ã‚“ã¸ã‚‰", "ã‚‚ã†ã‚ã„", "ã‚‚ã†ã˜ã‚“", "ã‚‚ã¯ã™ããƒ¼", "ã‚‚ã‚Šã¾ã‚“", "ã‚„ãƒ¼æ§˜", "ã‚„ã¶ã«ã‚‰ã¿", "ã‚„ã‚Šã¾ã‚“", "ã‚ˆã›ã°", "ã‚ˆã‚ã«ã‚„ã‚‹", "ã‚‰ã„ã³ã‚‡ã†", "ã‚‰ã„ç—…", "ã‚‰ãã«ã‚“ã¶ã‚‰ã", "ã‚‰ã‚“ã‹ã‚“", "ã‚Šã‚‡ã†ãã‚ƒã", "ã‚Šã‚‡ã†ã˜ã‚‡ã", "ã‚Šã‚‡ã†ã¯ã‚“", "ã‚Šã‚“ã‹ã‚“", "ã‚Šã‚“ã¡", "ã‚‹ã‚“ãºã‚“", "ã‚Œã„ã·", "ã‚ãƒ¼ãŸãƒ¼", "ã‚ã‚Šã“ã‚“", "ã‚ã‚Šã‚ã‚Š", "ã‚ã‚“ã±ã‚Š", "ã‚åŠ©", "æ„›æ¶²", "æš—æ®º", "ä¼Šå‹¢ã“ã˜ã", "æ…°å®‰å©¦", "è‚²ã¡ã‚ˆã‚Šæ°", "å¼•ã‹ã‚Œè€…", "æ·«å§¦", "æ·«è¡Œ", "æ·«å£²", "æ·«ç¸›", "æ·«èœœ", "æ·«æ¯›", "æ·«ä¹±", "é™°æ ¸", "é™°èŒ", "é™°å”‡", "é™°åš¢", "é™°æ¯›", "éš ã—æ’®ã‚Š", "éš äº¡å±‹", "å³ç¿¼", "è¶Šå¾Œã®ç±³ã¤ã", "å††å…‰", "æ´äº¤", "æ´åŠ©äº¤éš›", "æ±šç©¢å±‹", "æ²–ä»²ä»•", "æ²–ç¸„ç¤¾ä¼šå¤§è¡†å…š", "ä½•ã‹ã£ã·", "å«ã«ã‚„ã‚‹", "éå»å¸³", "è¡—å®£è»Š", "æ‹¡å¼µå“¡", "æ‹¡å¼µå›£", "æ ªå±‹", "å§¦æ·«", "å§¦é€š", "ç›£ç¦", "é¡”é¨", "é¡”å°„", "åŸºåœ°å¤–", "å¯„ã›å ´", "å¯„ã‚Šç›®", "æ°—é•ã„", "é¨ä¹—ä½", "é¬¼çŸ³æ›¼å­", "è™æ®º", "è™å¾…", "è™çŠ¯", "é€†æ´", "å·¨ã¡ã‚“", "å·¨ä¹³", "å¼·å§¦", "å¼·ç›—", "è„…è¿«", "é‡‘ç‰", "éŠ€è¡Œå£åº§", "æ„šéˆ", "ç¾¤ç›²", "çŠ¬æ®ºã—", "æ¸›ç¨æ—¥æœ¬", "å¾Œé€²å›½", "ä¹é£Ÿ", "äº¤å§¦", "å…¬æ˜å…š", "å¹¸ç¦ã®ç§‘å­¦", "å¹¸ç¦å®Ÿç¾å…š", "ç´…æ¯›äºº", "æ‹·å•", "å›½æ°‘æ–°å…š", "å·¦ç¿¼", "åœ¨æ—¥éŸ“å›½äºº", "æ®ºã™", "æ®ºã‚‹", "æ®ºå®³", "æ®ºäºº", "ä¸‰éŸ“å¾ä¼", "ä¸‰å›½äºº", "ä¸‰ä¹Ÿ", "å£«è¾²å·¥å•†", "å­å®®", "å­ç¨®", "æŒ‡ã¾ã‚“", "æ”¯é‚£äºº", "æ­»ã­", "æ­»å§¦", "æ­»ä½“", "æ°ã­", "æ°ã‚ˆã‚Šè‚²ã¡", "è‡ªæ…°", "è‡ªæ®º", "è‡ªé–‰ç—‡å…", "è‡ªæ°‘å…š", "å¤±ç¦", "å°„ç²¾", "ç¤¾ä¼šå…š", "æ‰‹ã“ã", "æ‰‹æ·«", "æ‰‹é¦–ã¡ã‚ƒã‚“", "é…’é¬¼è–”è–‡è–æ–—", "é¦–åˆ‡ã‚Š", "å—ç²¾", "å‘¨æ—‹å±‹", "è¥²æ’ƒ", "æ±ç”·å„ª", "ç£å§¦", "å‡ºä¼šã„ç³»", "ç´”ã¨ã‚", "å‡¦å¥³", "å¥³å­ä¾›", "å¥³éƒ", "å¨¼å©¦", "å°æ ¹", "å°ä½¿ã„", "å°æ—¥æœ¬", "å°ä¾¿", "-å°ä¾¿å°åƒ§", "å°‘å¹´é™¢", "ä¸Šæ–¹ã®ãœã„å…­", "æ¤ç‰©äººé–“", "å¿ƒèº«éšœå®³è€…", "æ–°å…šæ”¹é©", "æ–°å…šå¤§åœ°", "æ–°å…šæ—¥æœ¬", "æ–°å¹³æ°‘", "äººèº«å£²è²·", "æ€§å™¨", "æ€§äº¤", "æ€§èŠ¯ç•°å¸¸", "æ­£å¸¸ä½", "ç”Ÿã›ã‚‰", "ç”Ÿã¯ã‚", "ç²¾æ¶²", "ç²¾å­", "ç²¾ç¥ç•°å¸¸", "ç²¾ç¥è–„å¼±å…", "ç²¾è–„è€…", "é’å§¦", "èµ¤æ——", "è³æ°‘", "é®®äºº", "ç²—ã¡ã‚“", "å‰µä¾¡å­¦ä¼š", "æ—©æ¼", "ç›¸å§¦", "è¶³åˆ‡ã‚Š", "ä½“ä½", "ä½“ç½°", "å°æ¹¾ã¯ã’", "å¤§äººã®ãŠã‚‚ã¡ã‚ƒ", "å¤§äººã®é–¢ä¿‚", "å¤§éº»", "ç¬¬ä¸‰å›½", "ç”·æ±", "çŸ¥æµã®é…ã‚Œã¦ã„ã‚‹å­ä¾›", "çŸ¥æµé…ã‚Œ", "åœ°ã¾ã‚ã‚Š", "æ¥å§¦", "ç—´æ¼¢", "ç—´å¥³", "ç—´æ…‹", "ç—´å‘†", "é…æ¼", "ä¸­å…±", "ä¸­å›½å…±ç”£å…š", "ä¸­å‡ºã—", "ä¸­ç”°æ°", "ä»²ã ã—", "æœé®®å¾ä¼", "æ½®ãµã", "æ½®å¹", "èª¿æ•™", "ç›´ã‚ã©", "ç›´ã‚", "ä½é–‹ç™ºå›½", "å‰ƒæ¯›", "æ³¥æ£’", "æººã‚Œæ­»ã¬", "å¤©å®‰é–€äº‹ä»¶", "å¤©æ‰ã¨ç‹‚äººã¯ç´™ä¸€é‡", "è»¢å£²", "å± æ®º", "å± å½¹", "åœŸå·¦è¡›é–€", "åœŸäºº", "å¥´éš·", "æ±äºœç—…å¤«", "æ±å¤·", "æ±æ´‹é¬¼", "ç›—ã‚€", "ç›—æ’®", "ç›—è´", "çµ±ä¸€å”ä¼š", "è¸åˆ‡ç•ª", "åŒå’Œåœ°åŒº", "ç«¥è²", "ç‰¹æ®Šå­¦ç´š", "ç‰¹æ®Šå­¦æ ¡", "æ¯’æ®º", "å—é®®", "å—éƒ¨ã®ã—ã‚ƒã‘ã®é¼»ã¾ãŒã‚Š", "è»Ÿç¦", "äºŒç©´è²¬ã‚", "è‚‰ã³ã‚‰", "è‚‰ä¾¿å™¨", "è‚‰æ£’", "è‚‰å£º", "æ—¥æœ¬ã®ã¡ã¹ã£ã¨", "æ—¥æœ¬é¬¼å­", "æ—¥æœ¬å…±ç”£å…š", "ä¹³é¦–", "å°¿é“", "æ¿¡ã‚Œã¾ã‚“", "è¦—ã", "é¦¬é¹¿ã§ã‚‚ã¡ã‚‡ã‚“ã§ã‚‚", "é¦¬ä¸", "å»ƒäºº", "æ’ä¾¿", "æ’æ³„", "è²·æ˜¥", "å£²æ˜¥", "å£²å¥³", "ç™½ç—´", "ç™ºæƒ…æ¶²", "åŠå³¶äºº", "çŠ¯ã—", "çŠ¯ã™", "çŠ¯ã‚‹", "çŠ¯ç½ª", "ç•ªå¤ª", "éäºº", "è¡¨æ—¥æœ¬", "è²§ä¹³", "è²§è¾²", "ä¸æ²»ã®ç—…", "å¯Œå±±ã®ä¸‰åŠ©", "æµ®æµªå…", "æµ®æµªè€…", "éƒ¨è½", "é¢¨ä¿—", "ç³å°¿", "æ–‡ç›²", "å¤‰æ…‹", "åŒ…èŒ", "æ”¾å°¿", "æ³•è¼ªåŠŸ", "æš´å§¦", "æš´è¡Œ", "åŒ—é®®", "æ’²æ®º", "å‹ƒèµ·", "æœ¬æ°—æ±", "éº»è–¬", "ä¸‡å¼•ã", "æœªé–‹ç™ºå›½", "å¯†å£²", "å¯†è¼¸", "æ°‘ä¸»å…š", "æ°‘é€²å…š", "å©¿ã‚’ã¨ã‚‹", "å¨˜ã‚’ã‹ãŸã¥ã‘ã‚‹", "æ¯›å”", "æ¯›ç­‰", "ç›²æ„›", "ç›²äºº", "æœ¨ã£ç«¯å½¹äºº", "ç›®éšœã‚Š", "éŠå¥³å±‹", "ä¹±å§¦", "ä¹±äº¤", "åµå­", "è£ã³ã§ãŠ", "è£æ—¥æœ¬", "å‡Œè™", "å‡Œè¾±", "å‡ŒçŠ¯", "é™µè¾±", "è¼ªå§¦", "ç‚‰åˆ©", "ç‚‰ç†", "éœ²åŠ©", "å’Œå§¦", "è³„è³‚", "å–˜ã„ã§", "åªšè–¬", "å¬²ã‚‹", "å«ã‚‹", "æµ£è…¸", "çŒ¥è¤»", "ç¾ä¸¸", "ç©¢å¤š", "è‚›é–€", "è†£", "ã‚¯ãƒ³ãƒ‹", "asahinet.jp", "auone-net.jp", "biglobe.ne.jp", "båœ°åŒº", "commufa.jp", "dion.ne.jp", "docomo.ne.jp", "eonet.ne.jp", "ezweb.ne.jp", "fuck", "gmail.com", "gã™ã½ã£ã¨", "hotmail.co.jp", "live.jp", "manko", "nifty.com", "ocn.ne.jp", "pkga.jp", "plala.or.jp", "sex", "so-net.ne.jp", "softbank.ne.jp", "wakwak.com", "yahoo.co.jp",
                            # 1) from ng col (https://docs.google.com/spreadsheets/d/1RF3nzLGfaQeENwMXYQOR3Dw6bH7l7dJ9a6IPlbYW6l8/edit#gid=235398883)
                            "SEX", "ã†ã‚“ã“", "ã¡ã‚“ã¡ã‚“", "å¤©åŸç‡éŸ³ã§ã‚ã‚Œã°ã€ã€Œå¤©åŸç‡éŸ³ã€ã€Œå¤©åŸã€ã€Œç‡éŸ³ã€ãªã©è‡ªèº«ã®åå‰ã¯çµ¶å¯¾1ç•ªãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ä¸Šã§å¤§ããè¡¨ç¤ºã•ã‚Œã¦ã—ã¾ã†ã¯ãšãªã®ã§ã€å„ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è‡ªèº«ã®åå‰ã¯è¡¨ç¤ºã—ãªã„ã‚ˆã†ã«ã—ãŸã„ã§ã™",
                            # symbol
                            "http", "https", "#", ".", "/", ":", "@", "_", "b", "g", "s", "t", "90", "RQ", "RT", "co", "ou", "pV", "ã€", "ã€‚", "ã€Œ", "ã€", "ã‚", "ã", "ã“", "ã™", "ãš", "ã©", "ã­", "ã°", "ã‚‹", "ã‚“", "ä¸ƒ", "ä¸Š", "ä¸­", "å„", "å¼“", "å½“", "æ¥½", "ç‡", "ç•ª", "ç¨®", "èŒ¨", "éŸ³", "ï¼ˆ", "ï¼Ÿ", "ğŸ¥¸", "ğŸ¥º",
                            "0", "1", "2", "3", "4", "5", "6", "7",  "8", "9",
                            "a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k", "l", "m", "n", "o", "p", "q", "r", "s", "t", "u", "v", "w", "x", "y", "z",
                            "A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z",
                            "ï¼", "ï¼", "ï½", "ğŸ“¢", "ï¸", "ğŸ’•", "ğŸ¤", "ğŸ‘¶", "ğŸŒ¸",
                            "â€¼", "ã€", "ã€‘",

                            "â€¦", "â‰«", "â˜…", "â™ª",
                            "!", "Ã©", "ğŸ˜¢", "ğŸ™„", "ğŸ™†", "ğŸ™", "â€â™€",
                            "(", ")", "â“", "ã€€", "ãƒ»", "ï¼’", "ï¼•", "ï½¡", "ï½¥", "ï¾Ÿ", "ğŸ¤ ",
                            # default
                            'ã¦ã‚‹', 'ã„ã‚‹', 'ãªã‚‹', 'ã‚Œã‚‹', 'ã™ã‚‹', 'ã‚ã‚‹', 'ã“ã¨', 'ã“ã‚Œ', 'ã•ã‚“', 'ã—ã¦',   # OPT: stopwords.update()
                            'ãã‚Œã‚‹', 'ã‚„ã‚‹', 'ãã ã•ã‚‹', 'ãã†', 'ã›ã‚‹', 'ã—ãŸ',  'æ€ã†',
                            'ãã‚Œ', 'ã“ã“', 'ã¡ã‚ƒã‚“', 'ãã‚“', '', 'ã¦', 'ã«', 'ã‚’', 'ã¯', 'ã®', 'ãŒ', 'ã¨', 'ãŸ', 'ã—', 'ã§',
                            'ãªã„', 'ã‚‚', 'ãª', 'ã„', 'ã‹', 'ã®ã§', 'ã‚ˆã†', '', 'ã‚Œ', 'ã•', 'ãªã£',
        ])
    # print("stopwordSet =", stopwordSet)

    """ tweets = get_tweets(keyword, hashtag, lang)

    text = ""
    for tweet in tweets:
        # print(f"IMPORTANT: tweet = {tweet}")
        text += tweet._json["full_text"]
        # print("text:", text) """

    """tweets = get_tweets(keyword, hashtag, lang)

    if prev_data is not None:
        # tweets += prev_data
        tweets = prev_data + tweets
    # tweets = ["aaa bbb ccc", "bbb ccc ddd", "eee fff ggg"]
    # print(f"tweets = {tweets}")"""

    count_dict = dict()
    titles_dict = dict()
    # tweet id used to get its url (http://twitter.com/twitter/statuses/{id})
    ids_dict = dict()
    out_dict = dict()

    for tweet in tweets:
        """ # print(f"tweet = {tweet}")   # D
        # print(f"tweet._json = {tweet._json}")
        # print(f"tweet._json['full_text'] = {tweet._json['full_text']}")
        # print(f"tweet.text = {tweet.text}")
        # print(f"tweet.id = {tweet.id}") """

        # get text & id
        text = tweet["content"]
        id = tweet["id"]
        # print(f"HERE: text = {text}")

        # remove stop bfr split sentence into words
        for stopword in stopwordSet:
            text = text.replace(stopword, '')

        # words = text.split()   // ONLY work well in en
        # The Tagger object holds state about the dictionary.
        tagger = fugashi.Tagger()  # type: ignore
        words = [word.surface for word in tagger(text)]

        # print(text)
        # print(words)

        # if (word not in count_dict)
        # count_dict.

        for word in words:
            # print(word)

            """# remove stop aft split sentence into words
            if (word in stopwordSet):
                # print(f"this word {word} is a stopword")
                continue"""

            # count_dict[word] = ((word in count_dict) ? count_dict.get(word) : 0) + 1
            count = 0   # counter start from 0 by default
            titles = list()   # OPT: set()
            ids = list()   # OPT: set()

            if (word in count_dict):
                # set default value if value is None
                count = count_dict.get(word)
                titles = titles_dict.get(word)
                ids = ids_dict.get(word)

            # use back default value if None
            if count is None:
                count = 0
            if titles is None:
                titles = list()
            if ids is None:
                ids = list()

            # print("=====")
            # print(f"BFR: titles = {titles}")
            # avoid to add duplicate data (can't use set in JSONObject())
            if text not in titles:
                '''print(f"(text not in titles) = {(text not in titles)}")
                print(f"text = {text}")
                print(f"titles = {titles}")'''
                titles.append(text)   # OPT: add
            # avoid to add duplicate data (can't use set in JSONObject())
            if id not in ids:
                ids.append(id)   # OPT: add
            # print(f"AFR: titles = {titles}")

            count_dict[word] = count + 1
            titles_dict[word] = titles
            ids_dict[word] = ids
            # print(f"count_dict = {count_dict}")
            # print(f"titles_dict = {titles_dict}")

            out_dict = {
                "count": count_dict,
                "sample_title": titles_dict,
                "ids": ids_dict
            }
            # print(f"out_dict = {out_dict}")

    # return text
    return out_dict


def get_refined_tweet_list(keyword=None, hashtag=None):
    import pandas as pd   # make dataframe, export csv

    tweets = get_tweets(keyword, hashtag)

    refined_tweet_list = list()
    for tweet in tweets:
        # print(f"IMPORTANT: tweet = {tweet}")
        text = tweet._json["full_text"]
        # print("text:", text)

        refined_tweet = {'text': text,
                         'favorite_count': tweet.favorite_count,
                         'retweet_count': tweet.retweet_count,
                         'created_at': tweet.created_at}

        refined_tweet_list.append(refined_tweet)
        # print("refined_tweet_list:", refined_tweet_list)

    # make a dataframe of tweets with columns representing the different attributes of tweet
    df = pd.DataFrame(refined_tweet_list)
    df.to_csv('refined_tweets.csv')
    # print("df:", df)

    # return df
    return refined_tweet_list
